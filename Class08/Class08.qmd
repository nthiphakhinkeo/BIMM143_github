---
title: "Class08" 
author: Nicholas Thiphakhinkeo A17686679
format: pdf
---
# Save Input Data File
```{r}
fna.data <- "WisconsinCancer.csv"
```
# Storing
```{r}
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```
# Remove 1st Column
```{r}
wisc.data <- wisc.df[,-1]
```
# Diagnosis Vector
```{r}
diagnosis <- wisc.df[,1]
head(diagnosis)
```
# Benign v Malignant Count
```{r}
table(diagnosis)
```
# Q1. How Many Observations in this Dataset?
569 Observations
-------------------
# Q2. How many Malignant?
212 Cases
-------------------
# Q3. How many Variables in Data are Suffixed with '_mean'?
```{r}
mean_check <- length(grep("_mean", colnames(wisc.data)))
mean_check
```
# Checking Column Means and SD
```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```
# PCA 
```{r}
wisc.pr <- prcomp(wisc.data, scale=T)
summary(wisc.pr)
```
# Q4. What proportion of the original variance is captured by the first principal components (PC1)?
.4427

# Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
Cumulative proportion exceeds 70% at PC3 (0.72636). Therefore, three principal components (PC1, PC2, and PC3) are required to explain at least 70% of the original variance in the data.

# Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
Cumulative proportion exceeds 90% at PC7 (0.91010). Therefore, seven principal components (PC1-PC7) are required to explain at least 90% of the original variance in the data

# Biplot
```{r}
biplot(wisc.pr)
```
# Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
This plot is a hot mess, need to generate our own plots for better understanding.

# Main "PC Score Plot", "PC1vPC2 Plot"
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=as.factor(diagnosis), xlab="PC1",ylab="PC2")
```
# Q8. Generating PC1 and PC3 Plot
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=as.factor(diagnosis), xlab="PC1",ylab="PC3")
```
This plot is not as clearly separated as PC1 vs PC2

# Creating Data.Frame for ggplot
```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```
# Making Scatterplot using ggplot2
```{r}
library(ggplot2)
ggplot(df) +
  aes(PC1,PC2, col=df$diagnosis) +
  geom_point()
```
# Calculating Variance of Each Component
```{r}
pr.var <- wisc.pr$sdev
head(pr.var)
```
# Variance Explained by Each Principal Component: pve
```{r}
pr.var <- wisc.pr$sdev^2
pve <- pr.var/sum(pr.var)
pve
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
# Alternative Scree Plot of Same Data
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
# Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
loadings <- wisc.pr$rotation[, 1]
concave_points_loading <- loadings[names(wisc.data) == "concave.points_mean"]
head(concave_points_loading)
```
# Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
Cumulative proportion exceeds 80% at PC5 (0.84734). Therefore, seven principal components (PC1-PC5) are required to explain at least 80% of the original variance in the data.

# Hierarchical Clustering 
# Scale Function
```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method="complete")
plot(wisc.hclust)
```
# Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(h = 19, col="red",lty = 2)
```
h = 19

# Selecting Number of Clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h = 19)
table(wisc.hclust.clusters, diagnosis)
```
# Q12. find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
```{r}
wisc.hclust.clusters2 <- cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters2, diagnosis)
wisc.hclust.clusters10 <- cutree(wisc.hclust, k=10)
table(wisc.hclust.clusters10, diagnosis)
```



# Combining PCA and Clustering
```{r}
d <- dist(wisc.pr$x[,1:3])
hc <-hclust(d, method="ward.D2")
plot(hc)
```
# Cutree into 2 groups/branches
```{r}
grps <- cutree(hc,k=2)
```

```{r}
plot(wisc.pr$x, col=grps)
```
# Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
My favorite is ward.d2  it produces the cleanest and most balanced clustering.

Single
```{r}
wist.single.clust <- hclust(data.dist, method="single")
plot(wist.single.clust)
```
Average
```{r}
wist.average.clust <- hclust(data.dist, method="average")
plot(wist.average.clust)
```
Ward.D2
```{r}
wist.D2.clust <- hclust(data.dist, method="ward.D2")
plot(wist.D2.clust)
```
